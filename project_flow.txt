> Create project template by executing template.py file
> Write the code on setup.py and pyproject.toml to import local packages
> Create a venv, activate it and install the requirements from requirements.txt file
    - "conda create -n mlops python=3.10 -y"
    - "conda activate mlops"
    - add required modules to requirements.txt file
    - do "pip install -r requirements.txt"
> do "pip list" to ensure you have local packages installed

---------------------------------MongoDB Setup--------------------------------------
> Sign up on MongoDB Atlas and create a project
> From "create a cluster" tab, hit "create" then select M0 service keeping other services as default and hit "create deployment"
> Setup the name(aliasjid009) and password and then create DB user
> Go to "network access" and add ip address "0.0.0.0/0" so that we can access it from anywhere
> Go back to project >> "get connection string" >> "Drivers" >> {Driver:Python, Version 3.6 or later}
 - copy and save the connection string
> (Optional) Go and read about setup and pyproject file in "imp_things.txt" file then proceed with further steps
> create folder "notebook" >> add dataset to notebook folder >> create file "mongoDB_demo.ipynb" >> select python kernel
> Push your data to the MongoDB database from your python notebook
> Go to MongoDB Atlas >> Database >> browse collection >> see your data in key value format

---------------------------------Logging, Exception and Notebooks--------------------
> Write the logger file and test it on demo.py
> Write the exception file and test it on demo.py
> EDA and Feature Engg notebook added

----------------------------------Data Ingestion------------------------------------
> Before we work on "Data Ingestion" component >> Declare variables within constants.__init__.py file >> 
    add code to configuration.mongo_db_connections.py file and define the func for mondodb connection >> 
    Inside "data_access" folder, add code to proj1_data that will use mongo_db_connections.py
    to connect with DB, fetch data in key-val format and transform that to df >>
    add code to entity.config_entity.py file till DataIngestionConfig class >>
    add code to entity.artifact_entity.py file till DataIngestionArtifact class >>
    add code to components.data_ingestion.py file >> add code to training pipeline >> 
    run demo.py (set mongodb connection url first, see next step)
> To setup the connection url on mac(also work for windows), open bash/powershell terminal and run below command:
                        *** For Bash ***
    set: export MONGODB_URL="mongodb+srv://<username>:<password>......"
    check: echo $MONGODB_URL
                        *** For Powershell ***
    set: $env:MONGODB_URL = "mongodb+srv://<username>:<password>......"
    check: echo $env:MONGODB_URL

    To setup the connection url on Windows, open env variable setting option and add a new variable:
    Name: MONGODB_URL, Value = <url>
    Also add "artifact" dir to .gitignore file

------------------------Data Validation, Data Transformation & Model Trainer----------------------------

> Complete the work on utils.main_utils.py and config.schema.yaml file (add entire info about dataset for data validation step)
> Now work on the "Data Validation" component the way we did in step 17 for Data Ingestion. (Workflow mentioned below)
> Now work on the "Data Transformation" component the way we did in above step. (add estimator.py to entity folder)
> Now work on the "Model Trainer" component the way we did in above step. (add class to estimator.py in entity folder)

> Before moving to next component of Model Evaluation, some AWS services setup is needed:
      * Login to AWS console.
      * Keep region set as - us-east-1
      * Go to IAM >> Create new user (name: firstproj)
      * Attach policy >> select AdministratorAccess >> next >> create user
      * Go to the user >> Security Credentials >> Access Keys >> Create access key
      * Select CLI >> agree to condition >> next >> Create Access Key >> download csv file
      * Set env variables with above csv values using below method:
      ====================================================================================
         >> Set env var from bash terminal: <<
         export AWS_ACCESS_KEY_ID="AWS_ACCESS_KEY_ID"
         export AWS_SECRET_ACCESS_KEY="AWS_SECRET_ACCESS_KEY"
         >> Check env var from bash terminal: <<
         echo $AWS_ACCESS_KEY_ID
         echo $AWS_SECRET_ACCESS_KEY

         >> Set env var from powershell terminal: <<
         $env:AWS_ACCESS_KEY_ID="AWS_ACCESS_KEY_ID"
         $env:AWS_SECRET_ACCESS_KEY="AWS_SECRET_ACCESS_KEY"
         >> Check env var from powershell terminal: <<
         echo $env:AWS_ACCESS_KEY_ID
         echo $env:AWS_SECRET_ACCESS_KEY
      ====================================================================================
      * Now add the access key, secret key, region name to constants.__init__.py
      * Add code to src.configuration.aws_connection.py file (To work with AWS S3 service)
      * Ensure below info in constants.__init__.py file:
            MODEL_EVALUATION_CHANGED_THRESHOLD_SCORE: float = 0.02
            MODEL_BUCKET_NAME = "my-model-mlopsproj"
            MODEL_PUSHER_S3_KEY = "model-registry"
      * Go to S3 service >> Create bucket >> Region: us-east-1 >> General purpose >>
        Bucket Name: "my-model-mlopsproj" >> uncheck: "Block all public access" and acknowledge >>
        Hit Create Bucket
      * Now inside "src.aws_storage" code needs to be added for the configurations needed to pull 
        and push model from AWS S3 bucket. 
      * Inside "entity" dir we will have an "s3_estimator.py" file containing all the func to pull/push
        data from s3 bucket.